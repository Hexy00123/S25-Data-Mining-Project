{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "import joblib\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "import implicit\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "from journal_name_preprocessor import preprocess_journal_batch\n",
    "from category_preprocessor import preprocess_categories_and_text_batch, simplify_category_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: arxiv-metadata-oai-snapshot.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5333b7f5946b470096043c41b5e8216e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data loaded:\n",
      "Dataset({\n",
      "    features: ['title', 'abstract', 'categories', 'journal-ref'],\n",
      "    num_rows: 2720631\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data_path = 'arxiv-metadata-oai-snapshot.json'\n",
    "print(f\"Loading data from: {data_path}\")\n",
    "\n",
    "raw_data = datasets.load_dataset(\n",
    "    \"json\",\n",
    "    data_files=data_path,\n",
    "    split='train' # Load everything as train initially\n",
    ")\n",
    "\n",
    "# Select relevant columns (include original categories for now)\n",
    "raw_data = raw_data.select_columns(['title', 'abstract', 'categories', 'journal-ref'])\n",
    "print(\"Raw data loaded:\")\n",
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying journal preprocessing and filtering (min_samples=100)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3241464a43b74128b2dfbf490994c01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2720631 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0315dd160048401eb129cb69e4c53cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/2720631 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after removing empty/uncleanable journal refs: 873250\n",
      "Total unique cleaned journal names found: 138946\n",
      "Number of unique journals meeting threshold (>= 100 papers): 549\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b3efa61c364c76b87200462284aff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/873250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples after filtering by journal frequency: 577727\n"
     ]
    }
   ],
   "source": [
    "min_journal_samples = 100 # Threshold for credible journals\n",
    "print(f\"Applying journal preprocessing and filtering (min_samples={min_journal_samples})...\")\n",
    "\n",
    "# Apply journal cleaning using the external function via .map()\n",
    "# Note: .map might be faster for large datasets than list comprehension used before\n",
    "# Adjust num_proc based on available cores\n",
    "data_cleaned_journals = raw_data.map(\n",
    "    preprocess_journal_batch,\n",
    "    batched=True,\n",
    "    num_proc=4 # Adjust as needed\n",
    ")\n",
    "\n",
    "# Filter out rows where journal cleaning resulted in None\n",
    "data_filtered_journals = data_cleaned_journals.filter(\n",
    "    lambda x: x['journal_cleaned'] is not None,\n",
    "    num_proc=4 # Adjust as needed\n",
    ")\n",
    "print(f\"Rows after removing empty/uncleanable journal refs: {len(data_filtered_journals)}\")\n",
    "\n",
    "# Calculate journal counts and identify valid journals (requires converting a column to pandas or iterating)\n",
    "# Let's do it efficiently without full pandas conversion if possible\n",
    "journal_counts = defaultdict(int)\n",
    "for journal in data_filtered_journals['journal_cleaned']:\n",
    "    journal_counts[journal] += 1\n",
    "\n",
    "valid_journals = [j for j, count in journal_counts.items() if count >= min_journal_samples]\n",
    "valid_journal_set = set(valid_journals) # Use set for faster filtering lookup\n",
    "\n",
    "print(f\"Total unique cleaned journal names found: {len(journal_counts)}\")\n",
    "print(f\"Number of unique journals meeting threshold (>= {min_journal_samples} papers): {len(valid_journals)}\")\n",
    "\n",
    "# Filter the dataset to keep only rows with valid journals\n",
    "data_filtered = data_filtered_journals.filter(\n",
    "    lambda x: x['journal_cleaned'] in valid_journal_set,\n",
    "    num_proc=4 # Adjust as needed\n",
    ")\n",
    "print(f\"Total samples after filtering by journal frequency: {len(data_filtered)}\")\n",
    "\n",
    "# Create journal mappings\n",
    "journal_to_id = {journal: i for i, journal in enumerate(valid_journals)}\n",
    "id_to_journal = {i: journal for journal, i in journal_to_id.items()}\n",
    "num_valid_journals = len(valid_journals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying category simplification and text preprocessing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da09cc28cc0f4b66a2ead6471a5711ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/577727 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete.\n",
      "Dataset({\n",
      "    features: ['categories', 'journal-ref', 'journal_cleaned', 'categories_simplified_list', 'text_processed'],\n",
      "    num_rows: 577727\n",
      "})\n",
      "Number of unique SIMPLIFIED categories in final dataset: 36\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Apply Category & Text Preprocessing\n",
    "print(\"Applying category simplification and text preprocessing...\")\n",
    "\n",
    "# Use the external batch preprocessing function\n",
    "data_processed = data_filtered.map(\n",
    "    preprocess_categories_and_text_batch,\n",
    "    batched=True,\n",
    "    remove_columns=['title', 'abstract'], # Remove original text columns\n",
    "    num_proc=4 # Adjust as needed\n",
    ")\n",
    "print(\"Preprocessing complete.\")\n",
    "print(data_processed)\n",
    "\n",
    "# Find unique simplified categories from the *final* processed data\n",
    "all_simplified_cats_lists = data_processed['categories_simplified_list']\n",
    "unique_simplified_categories = sorted(list(set(cat for sublist in all_simplified_cats_lists for cat in sublist)))\n",
    "num_unique_simplified_categories = len(unique_simplified_categories)\n",
    "\n",
    "print(f\"Number of unique SIMPLIFIED categories in final dataset: {num_unique_simplified_categories}\")\n",
    "\n",
    "# Create simplified category mappings\n",
    "class2id_simplified = {cat: i for i, cat in enumerate(unique_simplified_categories)}\n",
    "id2class_simplified = {i: cat for i, cat in enumerate(unique_simplified_categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained classifier from: arxiv_category_classifier_logreg.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elisey/.pyenv/versions/3.12.7/lib/python3.12/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.2.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/elisey/.pyenv/versions/3.12.7/lib/python3.12/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.2.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elisey/.pyenv/versions/3.12.7/lib/python3.12/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.2.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/elisey/.pyenv/versions/3.12.7/lib/python3.12/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LabelBinarizer from version 1.2.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/elisey/.pyenv/versions/3.12.7/lib/python3.12/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator OneVsRestClassifier from version 1.2.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/elisey/.pyenv/versions/3.12.7/lib/python3.12/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.2.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classifier_path = 'arxiv_category_classifier_logreg.joblib'\n",
    "print(f\"Loading pre-trained classifier from: {classifier_path}\")\n",
    "try:\n",
    "    model_pipeline = joblib.load(classifier_path)\n",
    "    print(\"Classifier loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Classifier file not found at {classifier_path}.\")\n",
    "    print(\"Please ensure the classifier was trained and saved separately.\")\n",
    "    # Handle error appropriately, maybe raise Exception\n",
    "    model_pipeline = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading classifier: {e}\")\n",
    "    model_pipeline = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting categories for the dataset...\n",
      "Prediction complete. Time taken: 19.51 seconds.\n",
      "Predictions shape: (577727, 36), Type: <class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "if model_pipeline:\n",
    "    print(\"Predicting categories for the dataset...\")\n",
    "    # Ensure data is in the right format (list of strings)\n",
    "    texts_for_prediction = data_processed['text_processed']\n",
    "    start_time = time.time()\n",
    "    y_pred = model_pipeline.predict(texts_for_prediction)\n",
    "    end_time = time.time()\n",
    "    print(f\"Prediction complete. Time taken: {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Predictions shape: {y_pred.shape}, Type: {type(y_pred)}\")\n",
    "else:\n",
    "    print(\"Skipping prediction as classifier model was not loaded.\")\n",
    "    y_pred = None # Or handle error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing the Category-Journal interaction matrix from predictions...\n",
      "Processing 693913 predicted category instances...\n",
      "Category-Journal matrix construction complete.\n",
      "Matrix shape: (36, 549) (Categories x Journals)\n",
      "Number of non-zero interactions: 5038\n"
     ]
    }
   ],
   "source": [
    "if y_pred is not None:\n",
    "    print(\"Constructing the Category-Journal interaction matrix from predictions...\")\n",
    "\n",
    "    category_journal_matrix = lil_matrix((num_unique_simplified_categories, num_valid_journals), dtype=int)\n",
    "\n",
    "    # Get journal names corresponding to the predictions\n",
    "    journal_names_processed = data_processed['journal_cleaned']\n",
    "\n",
    "    # Iterate through predictions (efficiently using COO format)\n",
    "    y_pred_coo = y_pred.tocoo()\n",
    "    paper_indices = y_pred_coo.row\n",
    "    predicted_cat_indices = y_pred_coo.col\n",
    "\n",
    "    print(f\"Processing {len(paper_indices)} predicted category instances...\")\n",
    "    for paper_idx, category_idx in zip(paper_indices, predicted_cat_indices):\n",
    "        try:\n",
    "            journal_name = journal_names_processed[paper_idx] # Direct index lookup\n",
    "            if journal_name in journal_to_id: # Check if journal is valid (should be)\n",
    "                journal_id = journal_to_id[journal_name]\n",
    "                category_journal_matrix[category_idx, journal_id] += 1\n",
    "            # Else: This case should ideally not happen if filtering was correct\n",
    "        except IndexError:\n",
    "             print(f\"Error: Index {paper_idx} out of bounds for journal_names_processed (length {len(journal_names_processed)})\")\n",
    "             break # Stop if indexing fails\n",
    "\n",
    "    # Convert to CSR for efficiency\n",
    "    category_journal_matrix_csr = category_journal_matrix.tocsr()\n",
    "\n",
    "    print(\"Category-Journal matrix construction complete.\")\n",
    "    print(f\"Matrix shape: {category_journal_matrix_csr.shape} (Categories x Journals)\")\n",
    "    print(f\"Number of non-zero interactions: {category_journal_matrix_csr.nnz}\")\n",
    "\n",
    "    # Save the matrix based on predictions\n",
    "    joblib.dump(category_journal_matrix_csr, 'category_journal_matrix_PREDICTED.joblib')\n",
    "    joblib.dump(id2class_simplified, 'id2class_simplified.joblib') # Save mappings if not done elsewhere\n",
    "    joblib.dump(journal_to_id, 'journal_to_id.joblib')\n",
    "else:\n",
    "    print(\"Skipping matrix construction as predictions are unavailable.\")\n",
    "    category_journal_matrix_csr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing matrix and training ALS model...\n",
      "Initializing ALS model with factors=10, regularization=0.01, iterations=30, use_gpu=False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1be63d167844499e692244f1228b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS model training complete. Time taken: 0.05 seconds.\n"
     ]
    }
   ],
   "source": [
    "if category_journal_matrix_csr is not None:\n",
    "    print(\"Preparing matrix and training ALS model...\")\n",
    "    # Implicit ALS: users=Categories, items=Journals\n",
    "    # Input matrix: Categories x Journals\n",
    "    als_input_matrix = category_journal_matrix_csr.astype(np.float32)\n",
    "\n",
    "    # --- Parameters for ALS ---\n",
    "    factors = 10\n",
    "    regularization = 0.01\n",
    "    iterations = 30\n",
    "    calculate_training_loss = True\n",
    "    use_gpu = implicit.gpu.HAS_CUDA\n",
    "    # --- --- --- --- --- --- ---\n",
    "\n",
    "    print(f\"Initializing ALS model with factors={factors}, regularization={regularization}, iterations={iterations}, use_gpu={use_gpu}\")\n",
    "    model_als = implicit.als.AlternatingLeastSquares(\n",
    "        factors=factors,\n",
    "        regularization=regularization,\n",
    "        iterations=iterations,\n",
    "        calculate_training_loss=calculate_training_loss,\n",
    "        use_gpu=use_gpu,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    model_als.fit(als_input_matrix)\n",
    "    end_time = time.time()\n",
    "    print(f\"ALS model training complete. Time taken: {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    # Save the trained ALS model\n",
    "    model_als.save('als_model.npz')\n",
    "else:\n",
    "    print(\"Skipping ALS training as interaction matrix is unavailable.\")\n",
    "    model_als = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions defined.\n"
     ]
    }
   ],
   "source": [
    "if model_als:\n",
    "    def precision_recall_at_k(model, train_matrix, k=10):\n",
    "        \"\"\"\n",
    "        Calculate mean Precision@k and Recall@k for ALS model.\n",
    "        Assumes train_matrix is the ground truth (users x items).\n",
    "        \"\"\"\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        num_users = train_matrix.shape[0]\n",
    "\n",
    "        # Get all recommendations for all users\n",
    "        user_ids = np.arange(num_users)\n",
    "        # This can be memory intensive for many users/items\n",
    "        ids, scores = model.recommend(user_ids, train_matrix, N=k, filter_already_liked_items=False)\n",
    "\n",
    "        for i, user_id in enumerate(user_ids):\n",
    "            training_items = train_matrix[user_id].indices # Indices of items user interacted with\n",
    "            if len(training_items) == 0:\n",
    "                continue # Skip users with no interactions in the training data\n",
    "\n",
    "            recommended_items = ids[i]\n",
    "\n",
    "            # Calculate hits\n",
    "            hits = np.isin(recommended_items, training_items)\n",
    "            num_hits = np.sum(hits)\n",
    "\n",
    "            precisions.append(num_hits / k)\n",
    "            recalls.append(num_hits / len(training_items))\n",
    "\n",
    "        mean_precision = np.mean(precisions) if precisions else 0.0\n",
    "        mean_recall = np.mean(recalls) if recalls else 0.0\n",
    "        mean_f1 = (2 * mean_precision * mean_recall) / (mean_precision + mean_recall) if (mean_precision + mean_recall) > 0 else 0.0\n",
    "\n",
    "        return mean_precision, mean_recall, mean_f1\n",
    "\n",
    "    def ndcg_at_k(model, train_matrix, k=10):\n",
    "        \"\"\"\n",
    "        Calculate mean NDCG@k for ALS model.\n",
    "        \"\"\"\n",
    "        ndcgs = []\n",
    "        num_users = train_matrix.shape[0]\n",
    "        user_ids = np.arange(num_users)\n",
    "        ids, scores = model.recommend(user_ids, train_matrix, N=k, filter_already_liked_items=False)\n",
    "\n",
    "        for i, user_id in enumerate(user_ids):\n",
    "            training_items = train_matrix[user_id].indices\n",
    "            if len(training_items) == 0:\n",
    "                continue\n",
    "\n",
    "            recommended_items = ids[i]\n",
    "\n",
    "            # Create relevance array (1 if item was interacted with, 0 otherwise)\n",
    "            relevance = np.isin(recommended_items, training_items).astype(np.float32)\n",
    "\n",
    "            # DCG calculation\n",
    "            discounts = np.log2(np.arange(len(recommended_items)) + 2)\n",
    "            dcg = np.sum(relevance / discounts)\n",
    "\n",
    "            # IDCG calculation (ideal ranking)\n",
    "            ideal_relevance = np.ones_like(relevance) # Assume all interacted items are relevant=1\n",
    "            ideal_dcg = np.sum(ideal_relevance[:len(training_items)] / np.log2(np.arange(min(k, len(training_items))) + 2))\n",
    "\n",
    "            ndcgs.append(dcg / ideal_dcg if ideal_dcg > 0 else 0.0)\n",
    "\n",
    "        return np.mean(ndcgs) if ndcgs else 0.0\n",
    "\n",
    "    print(\"Evaluation functions defined.\")\n",
    "else:\n",
    "    print(\"Skipping evaluation setup as ALS model is unavailable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating ALS Recommendation Metrics...\n",
      "Evaluation complete. Time taken: 0.01 seconds.\n",
      "------------------------------\n",
      "Metrics @10:\n",
      "  Precision@10: 0.8636\n",
      "  Recall@10:    0.0780\n",
      "  F1-Score@10:  0.1431\n",
      "  NDCG@10:      0.8642\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "if model_als and als_input_matrix is not None:\n",
    "    print(\"\\nCalculating ALS Recommendation Metrics...\")\n",
    "    k_eval = 10 # Evaluate at K=10\n",
    "\n",
    "    start_time = time.time()\n",
    "    mean_precision, mean_recall, mean_f1 = precision_recall_at_k(model_als, als_input_matrix, k=k_eval)\n",
    "    mean_ndcg = ndcg_at_k(model_als, als_input_matrix, k=k_eval)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Evaluation complete. Time taken: {end_time - start_time:.2f} seconds.\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Metrics @{k_eval}:\")\n",
    "    print(f\"  Precision@{k_eval}: {mean_precision:.4f}\")\n",
    "    print(f\"  Recall@{k_eval}:    {mean_recall:.4f}\")\n",
    "    print(f\"  F1-Score@{k_eval}:  {mean_f1:.4f}\")\n",
    "    print(f\"  NDCG@{k_eval}:      {mean_ndcg:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "else:\n",
    "    print(\"Skipping metric calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example Recommendations: Category -> Journals ---\n",
      "\n",
      "Recommending top 10 journals for category: 'astro-ph' (ID: 3)\n",
      "----------------------------------------\n",
      "- PhysRevSTAccelBeams            (Score: 0.9807)\n",
      "- IntModPhysA9                   (Score: 0.9802)\n",
      "- PoSLattice                     (Score: 0.9088)\n",
      "- LettMathPhys                   (Score: 0.7240)\n",
      "- PhysicalChemistryChemicalPhysics (Score: 0.6769)\n",
      "- AdvTheorMathPhys               (Score: 0.6668)\n",
      "- RevMathPhys                    (Score: 0.5959)\n",
      "- ACSNano                        (Score: 0.5610)\n",
      "- ICLR                           (Score: 0.5238)\n",
      "- symblog                        (Score: 0.5134)\n",
      "----------------------------------------\n",
      "\n",
      "Recommending top 10 journals for category: 'cs' (ID: 10)\n",
      "----------------------------------------\n",
      "- PhysNuclPartPhys               (Score: 0.6794)\n",
      "- BrazPhys                       (Score: 0.5735)\n",
      "- Nonlinearity                   (Score: 0.4479)\n",
      "- ModPhysLett                    (Score: 0.4281)\n",
      "- StatistPhys                    (Score: 0.4164)\n",
      "- CERNYellowReportCERNpp         (Score: 0.4150)\n",
      "- MonatshMath                    (Score: 0.3951)\n",
      "- ClassQuantGrav                 (Score: 0.3938)\n",
      "- TheAstrophysicalJournalVolumeIssue2articleidpp (Score: 0.3863)\n",
      "- NewPhys9                       (Score: 0.3589)\n",
      "----------------------------------------\n",
      "\n",
      "Recommending top 10 journals for category: 'math' (ID: 20)\n",
      "----------------------------------------\n",
      "- IEEERoboticsandAutomationLetters (Score: 0.4101)\n",
      "- LowTempPhys                    (Score: 0.3788)\n",
      "- ApJ                            (Score: 0.3753)\n",
      "- SciRep7                        (Score: 0.3709)\n",
      "- OpticsLetters                  (Score: 0.3689)\n",
      "- AstropartPhys                  (Score: 0.3681)\n",
      "- PASP                           (Score: 0.3345)\n",
      "- PhysNuclPartPhys               (Score: 0.3002)\n",
      "- JINST                          (Score: 0.2982)\n",
      "- PhysRevApplied                 (Score: 0.2973)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if model_als and als_input_matrix is not None:\n",
    "    print(\"\\n--- Example Recommendations: Category -> Journals ---\")\n",
    "    def recommend_journals_for_category(category_name, model, user_item_matrix, N=10):\n",
    "        if category_name not in class2id_simplified:\n",
    "            print(f\"Error: Category '{category_name}' not found.\")\n",
    "            return\n",
    "        category_id = class2id_simplified[category_name]\n",
    "        print(f\"\\nRecommending top {N} journals for category: '{category_name}' (ID: {category_id})\")\n",
    "        ids, scores = model.recommend(category_id, user_item_matrix[category_id], N=N) # Pass the category's row\n",
    "        print(\"-\" * 40)\n",
    "        for journal_id, score in zip(ids, scores):\n",
    "            journal_name = id_to_journal.get(journal_id, \"Unknown Journal\")\n",
    "            print(f\"- {journal_name:<30} (Score: {score:.4f})\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    recommend_journals_for_category('astro-ph', model_als, als_input_matrix, N=10)\n",
    "    recommend_journals_for_category('cs', model_als, als_input_matrix, N=10)\n",
    "    recommend_journals_for_category('math', model_als, als_input_matrix, N=10)\n",
    "else:\n",
    "    print(\"\\nSkipping category->journal recommendations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example Recommendations: Journal -> Similar Journals ---\n",
      "\n",
      "Finding top 10 journals similar to: 'PhysRevLett' (ID: 7)\n",
      "----------------------------------------\n",
      "- PhysRev                        (Score: 0.9997)\n",
      "- IntModPhys                     (Score: 0.9995)\n",
      "- EurPhys                        (Score: 0.9993)\n",
      "- PhysicalReview                 (Score: 0.9989)\n",
      "- EurophysLett                   (Score: 0.9983)\n",
      "- PhysRevResearch2               (Score: 0.9970)\n",
      "- NewPhys                        (Score: 0.9966)\n",
      "- PhysConfSer                    (Score: 0.9964)\n",
      "- EurPhysPlus                    (Score: 0.9941)\n",
      "- SciPostPhys                    (Score: 0.9929)\n",
      "----------------------------------------\n",
      "\n",
      "Finding top 10 journals similar to: 'Astrophys' (ID: 3)\n",
      "----------------------------------------\n",
      "- MonNotRoyAstronSoc             (Score: 0.9903)\n",
      "- SciChinaPhysMechAstron         (Score: 0.9901)\n",
      "- AstronAstrophys                (Score: 0.9495)\n",
      "- NuclInstrumMeth                (Score: 0.8429)\n",
      "- AstrophysicsandSpaceScience    (Score: 0.7954)\n",
      "- AIPConferenceProceedings       (Score: 0.7702)\n",
      "- Mathematics                    (Score: 0.7501)\n",
      "- ApJ                            (Score: 0.7010)\n",
      "- PhysRev3                       (Score: 0.6973)\n",
      "- MNRAS                          (Score: 0.6935)\n",
      "----------------------------------------\n",
      "\n",
      "Finding top 10 journals similar to: 'JHEP' (ID: 4)\n",
      "----------------------------------------\n",
      "- CommunTheorPhys                (Score: 0.9855)\n",
      "- Phys                           (Score: 0.9855)\n",
      "- PhysicsLetters                 (Score: 0.9831)\n",
      "- AIPConfProc                    (Score: 0.9679)\n",
      "- PhysRev5                       (Score: 0.9673)\n",
      "- SciPostPhys                    (Score: 0.9652)\n",
      "- PhysConfSer                    (Score: 0.9589)\n",
      "- EurPhysPlus                    (Score: 0.9545)\n",
      "- PhysRev                        (Score: 0.9539)\n",
      "- InternationalJournalofModernPhysics (Score: 0.9529)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if model_als:\n",
    "    print(\"\\n--- Example Recommendations: Journal -> Similar Journals ---\")\n",
    "    def find_similar_journals(journal_name, model, N=10):\n",
    "        journal_name_lower = journal_name.lower()\n",
    "        found_id = None\n",
    "        if journal_name in journal_to_id:\n",
    "             found_id = journal_to_id[journal_name]\n",
    "        else: # Try partial match\n",
    "             matches = [j for j in journal_to_id if journal_name_lower in j.lower()]\n",
    "             if len(matches) == 1:\n",
    "                 journal_name = matches[0]\n",
    "                 found_id = journal_to_id[journal_name]\n",
    "                 print(f\"(Matched '{journal_name}')\")\n",
    "             elif len(matches) > 1:\n",
    "                  print(f\"Ambiguous journal name '{journal_name}'. Matches: {matches}\")\n",
    "                  return\n",
    "             else:\n",
    "                 print(f\"Error: Journal like '{journal_name}' not found.\")\n",
    "                 return\n",
    "\n",
    "        print(f\"\\nFinding top {N} journals similar to: '{journal_name}' (ID: {found_id})\")\n",
    "        ids, scores = model.similar_items(found_id, N=N+1)\n",
    "        print(\"-\" * 40)\n",
    "        for other_journal_id, score in zip(ids, scores):\n",
    "            if other_journal_id != found_id:\n",
    "                other_journal_name = id_to_journal.get(other_journal_id, \"Unknown Journal\")\n",
    "                print(f\"- {other_journal_name:<30} (Score: {score:.4f})\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    find_similar_journals('PhysRevLett', model_als, N=10)\n",
    "    find_similar_journals('Astrophys', model_als, N=10)\n",
    "    find_similar_journals('JHEP', model_als, N=10)\n",
    "else:\n",
    "    print(\"\\nSkipping similar journal recommendations.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
